{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a73b581",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:06.496736Z",
     "iopub.status.busy": "2021-11-23T06:18:06.495897Z",
     "iopub.status.idle": "2021-11-23T06:18:08.447086Z",
     "shell.execute_reply": "2021-11-23T06:18:08.445624Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.271003Z"
    },
    "papermill": {
     "duration": 1.998639,
     "end_time": "2021-11-23T06:18:08.447355",
     "exception": false,
     "start_time": "2021-11-23T06:18:06.448716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        pass\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02267502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:08.556301Z",
     "iopub.status.busy": "2021-11-23T06:18:08.555220Z",
     "iopub.status.idle": "2021-11-23T06:18:11.738039Z",
     "shell.execute_reply": "2021-11-23T06:18:11.737386Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.416977Z"
    },
    "papermill": {
     "duration": 3.242024,
     "end_time": "2021-11-23T06:18:11.738240",
     "exception": false,
     "start_time": "2021-11-23T06:18:08.496216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "SIZE = 256\n",
    "size = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 15\n",
    "PATIENCE = 4\n",
    "LR_RATE = 0.005\n",
    "\n",
    "wb = 0\n",
    "if wb == 1:\n",
    "    wandb.init(project=\"Tim_test\", name = 'effnet-b4 ensemble 233131113' , entity=\"ct_01\",\n",
    "               config = {  \"learning_rate\": LR_RATE,\n",
    "                \"epoch\": EPOCH,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"resized\": size,\n",
    "                \"model\": \"effnet-b4 with pretrained\",\n",
    "                \"augment\": \"2:3:3, 1:3:1, 1:1:3\",\n",
    "                \"weight\": \"1:0.5:0.5\"})\n",
    "    \n",
    "    #wandb.config = {  \"learning_rate\": 0.001,\n",
    "     #           \"epoch\": EPOCH,\n",
    "      #          \"batch_size\": BATCH_SIZE,\n",
    "       #         \"resized\": size }\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca70ec8",
   "metadata": {
    "papermill": {
     "duration": 0.030042,
     "end_time": "2021-11-23T06:18:11.798769",
     "exception": false,
     "start_time": "2021-11-23T06:18:11.768727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# fixed seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421584d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:11.865238Z",
     "iopub.status.busy": "2021-11-23T06:18:11.864325Z",
     "iopub.status.idle": "2021-11-23T06:18:11.871375Z",
     "shell.execute_reply": "2021-11-23T06:18:11.870761Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.582524Z"
    },
    "papermill": {
     "duration": 0.043163,
     "end_time": "2021-11-23T06:18:11.871554",
     "exception": false,
     "start_time": "2021-11-23T06:18:11.828391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 887\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d41f3",
   "metadata": {
    "papermill": {
     "duration": 0.03036,
     "end_time": "2021-11-23T06:18:11.931823",
     "exception": false,
     "start_time": "2021-11-23T06:18:11.901463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# load picture function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10ee1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:12.003160Z",
     "iopub.status.busy": "2021-11-23T06:18:12.002063Z",
     "iopub.status.idle": "2021-11-23T06:18:12.004425Z",
     "shell.execute_reply": "2021-11-23T06:18:12.005058Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.596391Z"
    },
    "papermill": {
     "duration": 0.044038,
     "end_time": "2021-11-23T06:18:12.005254",
     "exception": false,
     "start_time": "2021-11-23T06:18:11.961216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "def load_picture_data(path):\n",
    "    path0 = path\n",
    "    files = listdir(path0)\n",
    "    files.sort()\n",
    "    #print(files)\n",
    "    #stop\n",
    "    all_images = []\n",
    "    imges_var = []\n",
    "    label = []\n",
    "    cnt=0\n",
    "    for every_image in files:\n",
    "        cnt += 1\n",
    "        #if cnt > 300: break\n",
    "        fullpath = join(path0, every_image)\n",
    "        if '.jpg' in every_image:\n",
    "            #print('haha')\n",
    "            #im = Image.open(fullpath)\n",
    "            img = cv2.imread(fullpath)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            #img = ((img-img.min())/(img.max()-img.min())*255).astype(np.uint8)  # min max\n",
    "            #print(img.dtype)\n",
    "            #new_img = img_transform(im)\n",
    "            \n",
    "            ######### blurring#####################################\n",
    "            transeImg = cv2.Laplacian(img, cv2.CV_64F)#############\n",
    "            imgVar = transeImg.var() ##################\n",
    "            imges_var.append(imgVar)\n",
    "            ######################################\n",
    "            #new_img = im.resize((size, size), Image.BILINEAR)  # change to the same size\n",
    "            \n",
    "            ###############\n",
    "            # use CLAHE####\n",
    "            ###############\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            #img = clahe.apply(img)\n",
    "            \n",
    "            #res = np.hstack((img, cl1))\n",
    "            #if cnt <= 5: cv2.imwrite(every_image, res)\n",
    "\n",
    "            img = np.array(img)  # image类转 numpy\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(img.shape) == 2:  # gray\n",
    "                #print('here', path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "                # print(every_image, img.shape)\n",
    "                \n",
    "            all_images.append(img)\n",
    "            #imges_var.append(imageVar)\n",
    "           \n",
    "                \n",
    "    print('load data over')\n",
    "\n",
    "    data = np.array(all_images)\n",
    "    #print(data.shape)\n",
    "    #label = pd.read_csv('_info.csv')\n",
    "\n",
    "    return data, np.array(imges_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbea642",
   "metadata": {
    "papermill": {
     "duration": 0.029738,
     "end_time": "2021-11-23T06:18:12.064897",
     "exception": false,
     "start_time": "2021-11-23T06:18:12.035159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# augment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87f6d4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:12.136441Z",
     "iopub.status.busy": "2021-11-23T06:18:12.135358Z",
     "iopub.status.idle": "2021-11-23T06:18:12.137985Z",
     "shell.execute_reply": "2021-11-23T06:18:12.138562Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.610732Z"
    },
    "papermill": {
     "duration": 0.044357,
     "end_time": "2021-11-23T06:18:12.138751",
     "exception": false,
     "start_time": "2021-11-23T06:18:12.094394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment(x_data, is_train, augment_flag):\n",
    "    img_transform = transforms.Compose([\n",
    "        #transforms.Resize((SIZE, SIZE)),\n",
    "        transforms.CenterCrop((512, 384)),\n",
    "        transforms.Resize((size, size), Image.BILINEAR),\n",
    "        #transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    ])\n",
    "    img_transform2 = transforms.Compose([\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.Resize((SIZE, SIZE)),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomResizedCrop(size), #, scale=(0.2, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img_transform3 = transforms.Compose([\n",
    "        #transforms.Resize((SIZE, SIZE)),\n",
    "        transforms.RandomResizedCrop(size),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    other_img_transform = transforms.Compose([\n",
    "        #transforms.Resize((SIZE, SIZE)),\n",
    "        transforms.Resize((size, size), Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    x_data_new=[]\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        #print(x_train[i].shape)\n",
    "        tmp = Image.fromarray(x_data[i])\n",
    "        #x_train[i] = img_transform(tmp)\n",
    "        if is_train==1 and augment_flag==1: x_data_new.append(np.array(img_transform(tmp)))\n",
    "        elif is_train==2 and augment_flag==1: x_data_new.append(np.array(img_transform2(tmp)))\n",
    "        elif is_train==3 and augment_flag==1: x_data_new.append(np.array(img_transform3(tmp)))\n",
    "        #elif is_train and augment_flag==2: x_data_new.append(np.array(img_transform2(tmp)))\n",
    "        #if is_train: x_data_new.append(np.array(img_transform(tmp))/255)\n",
    "        else: x_data_new.append(np.array(other_img_transform(tmp)))\n",
    "    #print(x_data_new[0])        \n",
    "    return np.array(x_data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21038e4a",
   "metadata": {
    "papermill": {
     "duration": 0.030056,
     "end_time": "2021-11-23T06:18:12.198339",
     "exception": false,
     "start_time": "2021-11-23T06:18:12.168283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b3c764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:12.268048Z",
     "iopub.status.busy": "2021-11-23T06:18:12.267017Z",
     "iopub.status.idle": "2021-11-23T06:18:32.326663Z",
     "shell.execute_reply": "2021-11-23T06:18:32.327632Z",
     "shell.execute_reply.started": "2021-11-23T06:17:18.628829Z"
    },
    "papermill": {
     "duration": 20.099795,
     "end_time": "2021-11-23T06:18:32.328062",
     "exception": false,
     "start_time": "2021-11-23T06:18:12.228267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data over\n",
      "load data over\n",
      "(1200, 512, 512, 3) (1200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #####################3\n",
    "    \n",
    "    #################\n",
    "    \n",
    "    data = 0\n",
    "    valid_data = 0\n",
    "    label = 0\n",
    "    \n",
    "    #train_path = '../input/case2-data/resized_train_data/resized_train_data'\n",
    "    #valid_path = '../input/case2-data/resized_valid_data'\n",
    "    \n",
    "    train_path = '../input/stacking-data/stacking_data/train'\n",
    "    valid_path = '../input/stacking-data/stacking_data/valid'\n",
    "    \n",
    "    data, data_var = load_picture_data(train_path)\n",
    "    valid_data, valid_data_var = load_picture_data(valid_path)\n",
    "    \n",
    "    \n",
    "    #idx = np.argsort(data_var)\n",
    "    #print(np.sort(data_var)[0:50])\n",
    "    \n",
    "    valid_dir = listdir(valid_path)\n",
    "    valid_dir.sort()\n",
    "\n",
    "    \n",
    "    label = pd.read_csv('../input/case2-data/_info.csv')\n",
    "    #for i in range(10):\n",
    "     #   print(label.loc[idx[i]])\n",
    "    label = label.drop(columns='FileID')\n",
    "    label = np.array(label)\n",
    "    label = np.argmax(label, axis=1)\n",
    "      \n",
    "    print(data.shape, label.shape)\n",
    "    \n",
    "    #print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44771b0f",
   "metadata": {
    "papermill": {
     "duration": 0.031617,
     "end_time": "2021-11-23T06:18:32.392742",
     "exception": false,
     "start_time": "2021-11-23T06:18:32.361125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# split & to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87bd06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:32.464004Z",
     "iopub.status.busy": "2021-11-23T06:18:32.463325Z",
     "iopub.status.idle": "2021-11-23T06:18:32.996661Z",
     "shell.execute_reply": "2021-11-23T06:18:32.997522Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.451735Z"
    },
    "papermill": {
     "duration": 0.573039,
     "end_time": "2021-11-23T06:18:32.997797",
     "exception": false,
     "start_time": "2021-11-23T06:18:32.424758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([280, 280, 280]))\n",
      "(array([0, 1, 2]), array([120, 120, 120]))\n",
      "(560, 512, 512, 3) (560,)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.3, stratify=label, random_state=seed)\n",
    "    #x_train, y_train = data.copy(), label.copy()\n",
    "    \n",
    "    add_ill_data_id = np.where(y_train>0) # augment on typical/atypical data\n",
    "    #print(add_ill_data_id)\n",
    "    add_ill_data = x_train[add_ill_data_id]\n",
    "    add_ill_label = y_train[add_ill_data_id]\n",
    "    \n",
    "\n",
    "    print(np.unique(y_train, return_counts=True))\n",
    "    print(np.unique(y_test, return_counts=True))\n",
    "    \n",
    "    print(add_ill_data.shape, add_ill_label.shape)\n",
    "    \n",
    "    \n",
    "    #print(np.unique(y_valid, return_counts=True))\n",
    "    #st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5bf61",
   "metadata": {
    "papermill": {
     "duration": 0.031061,
     "end_time": "2021-11-23T06:18:33.060927",
     "exception": false,
     "start_time": "2021-11-23T06:18:33.029866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# add typical/atypical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815cd795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:33.133183Z",
     "iopub.status.busy": "2021-11-23T06:18:33.131695Z",
     "iopub.status.idle": "2021-11-23T06:18:34.084189Z",
     "shell.execute_reply": "2021-11-23T06:18:34.083587Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.453894Z"
    },
    "papermill": {
     "duration": 0.992426,
     "end_time": "2021-11-23T06:18:34.084346",
     "exception": false,
     "start_time": "2021-11-23T06:18:33.091920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 512, 512, 3) (280, 512, 512, 3)\n",
      "(280,) (280,)\n"
     ]
    }
   ],
   "source": [
    "    x_train_original = x_train.copy()\n",
    "    y_train_original = y_train.copy()\n",
    "    \n",
    "    x_train_typical = x_train.copy()\n",
    "    y_train_typical = y_train.copy()\n",
    "    x_train_atypical = x_train.copy()\n",
    "    y_train_atypical = y_train.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    typical_data_id = np.where(y_train==1) # more augment on atypical data\n",
    "    atypical_data_id = np.where(y_train>1) # more augment on atypical data\n",
    "    \n",
    "    typical_data = x_train_typical[typical_data_id]\n",
    "    atypical_data = x_train_atypical[atypical_data_id]\n",
    "    \n",
    "    typical_label = y_train_typical[typical_data_id]\n",
    "    atypical_label = y_train_atypical[atypical_data_id]\n",
    "    \n",
    "    \n",
    "    #non_atypical_id = np.where(y_train<2)\n",
    "    #non_typical_id = np.where(y_train!=1)\n",
    "    #x_train_typical = x_train_typical[non_atypical_id] # only has 0, 1\n",
    "    #y_train_typical = y_train_typical[non_atypical_id] # only has 0, 1\n",
    "    #x_train_atypical = x_train_atypical[non_typical_id] # only has 0, 2\n",
    "    #y_train_atypical = y_train_atypical[non_typical_id] # only has 0, 2\n",
    "    \n",
    "    print(typical_data.shape, atypical_data.shape)\n",
    "    print(typical_label.shape, atypical_label.shape)\n",
    "    #print(typical_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf1900",
   "metadata": {
    "papermill": {
     "duration": 0.03153,
     "end_time": "2021-11-23T06:18:34.149339",
     "exception": false,
     "start_time": "2021-11-23T06:18:34.117809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# rotation & augment(choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d08dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:34.223231Z",
     "iopub.status.busy": "2021-11-23T06:18:34.222246Z",
     "iopub.status.idle": "2021-11-23T06:18:46.214357Z",
     "shell.execute_reply": "2021-11-23T06:18:46.215266Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.456045Z"
    },
    "papermill": {
     "duration": 12.034665,
     "end_time": "2021-11-23T06:18:46.215598",
     "exception": false,
     "start_time": "2021-11-23T06:18:34.180933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2240, 3, 224, 224) (2240,)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    #print(x_train.shape)\n",
    "    augment_flag = 1  ###### change this one\n",
    "    \n",
    "    x_train_aug = augment(x_train, 1, augment_flag)\n",
    "    #x_train_aug2 = augment(x_train, 3, augment_flag)\n",
    "    \n",
    "    \n",
    "    x_train = augment(x_train, 1, 0)\n",
    "    x_test = augment(x_test, 0, 0)\n",
    "    #x_valid = augment(x_valid, 0, 0)\n",
    "    valid_data = augment(valid_data, 0, 0)\n",
    "    \n",
    "    add_ill_data_first = augment(add_ill_data, 2, augment_flag) # augment on illness data\n",
    "    #add_ill_data_second = augment(add_ill_data, 3, augment_flag) # augment on illness data\n",
    "    #atypical_data_first = augment(atypical_data, 3, augment_flag) # augment on atypical data\n",
    "    \n",
    "    \n",
    "    y_train_aug = y_train.copy()\n",
    "    \n",
    "    if augment_flag == 1:    # *3 datas\n",
    "        x_train = np.concatenate([x_train, x_train_aug])\n",
    "        y_train = np.concatenate([y_train, y_train_aug])\n",
    "        \n",
    "        #x_train = np.concatenate([x_train, x_train_aug2])\n",
    "        #y_train = np.concatenate([y_train, y_train_aug])\n",
    "        \n",
    "        x_train = np.concatenate([x_train, add_ill_data_first]) # add illness data(augment)\n",
    "        y_train = np.concatenate([y_train, add_ill_label])\n",
    "        #x_train = np.concatenate([x_train, atypical_data_first]) # add atypical data\n",
    "        #y_train = np.concatenate([y_train, atypical_label])\n",
    "            \n",
    "            \n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y_train)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape)\n",
    "    #stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7d24c",
   "metadata": {
    "papermill": {
     "duration": 0.0321,
     "end_time": "2021-11-23T06:18:46.281331",
     "exception": false,
     "start_time": "2021-11-23T06:18:46.249231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# augment on typical/atypical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4391b49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:46.351325Z",
     "iopub.status.busy": "2021-11-23T06:18:46.350146Z",
     "iopub.status.idle": "2021-11-23T06:18:57.428801Z",
     "shell.execute_reply": "2021-11-23T06:18:57.427622Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.458191Z"
    },
    "papermill": {
     "duration": 11.115151,
     "end_time": "2021-11-23T06:18:57.429043",
     "exception": false,
     "start_time": "2021-11-23T06:18:46.313892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 3, 224, 224) (1400,)\n",
      "(1400, 3, 224, 224) (1400,)\n"
     ]
    }
   ],
   "source": [
    "    augment_flag = 1  ###### change this one\n",
    "    \n",
    "    x_train_typical_aug = augment(typical_data, 1, augment_flag)\n",
    "    x_train_atypical_aug = augment(atypical_data, 1, augment_flag)\n",
    " \n",
    "    another_x_train_typical_aug = augment(typical_data, 2, augment_flag)\n",
    "    another_x_train_atypical_aug = augment(atypical_data, 2, augment_flag)\n",
    "    \n",
    "    #third_x_train_typical_aug = augment(typical_data, 3, augment_flag)\n",
    "    #third_x_train_atypical_aug = augment(atypical_data, 3, augment_flag)\n",
    "    \n",
    "    #x_train_original_ = augment(x_train_original, 1, 0)\n",
    "    x_train_typical = augment(x_train_typical, 1, 0)\n",
    "    x_train_atypical = augment(x_train_atypical, 1, 0)\n",
    "    \n",
    "    #y_train_aug = y_train.copy()\n",
    "    \n",
    "    if augment_flag == 1:    # *3 datas\n",
    "        x_train_typical = np.concatenate([x_train_typical, x_train_typical_aug, another_x_train_typical_aug])\n",
    "        x_train_atypical = np.concatenate([x_train_atypical, x_train_atypical_aug, another_x_train_atypical_aug])\n",
    "        \n",
    "        y_train_typical = np.concatenate([y_train_typical, typical_label, typical_label])\n",
    "        y_train_atypical = np.concatenate([y_train_atypical, atypical_label, atypical_label])\n",
    "        \n",
    "        ###################### 1:1:4 & 1:4:1 aug\n",
    "        #x_train_atypical = np.concatenate([x_train_atypical, third_x_train_atypical_aug])\n",
    "        #y_train_atypical = np.concatenate([y_train_atypical, atypical_label])\n",
    "        #x_train_typical = np.concatenate([x_train_typical, third_x_train_typical_aug])\n",
    "        #y_train_typical = np.concatenate([y_train_typical, typical_label])\n",
    "        \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(x_train_typical)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(x_train_atypical)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y_train_typical)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y_train_atypical)\n",
    "    \n",
    "    \n",
    "    print(x_train_typical.shape, y_train_typical.shape)\n",
    "    print(x_train_atypical.shape, y_train_atypical.shape)\n",
    "    #for item in y_train_atypical:\n",
    "     #   print(item, end=' ')\n",
    "    #print(y_train_typical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e75061",
   "metadata": {
    "papermill": {
     "duration": 0.033302,
     "end_time": "2021-11-23T06:18:57.495083",
     "exception": false,
     "start_time": "2021-11-23T06:18:57.461781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f5d378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:57.569249Z",
     "iopub.status.busy": "2021-11-23T06:18:57.564576Z",
     "iopub.status.idle": "2021-11-23T06:18:59.211760Z",
     "shell.execute_reply": "2021-11-23T06:18:59.212777Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.460543Z"
    },
    "papermill": {
     "duration": 1.684709,
     "end_time": "2021-11-23T06:18:59.213065",
     "exception": false,
     "start_time": "2021-11-23T06:18:57.528356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2240, 3, 224, 224])\n",
      "torch.Size([360, 3, 224, 224])\n",
      "torch.Size([150, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    x_train = torch.tensor(x_train).to(dtype=torch.float16)\n",
    "    x_test = torch.tensor(x_test).to(dtype=torch.float16)\n",
    "    #x_valid = torch.tensor(x_valid).to(dtype=torch.float16)\n",
    "    valid_data = torch.tensor(valid_data).to(dtype=torch.float16)\n",
    "\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "    #y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "    #x_train = x_train.permute(0, 3, 1, 2)  # shape:[10000, 3, 32, 32]\n",
    "    #x_test = x_test.permute(0, 3, 1, 2)\n",
    "    #x_valid = x_valid.permute(0, 3, 1, 2)\n",
    "    #valid_data = valid_data.permute(0, 3, 1, 2)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    #print(x_valid.shape)\n",
    "    print(valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47a2ff",
   "metadata": {
    "papermill": {
     "duration": 0.033478,
     "end_time": "2021-11-23T06:18:59.283108",
     "exception": false,
     "start_time": "2021-11-23T06:18:59.249630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# to tensor (ty/aty..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1a5637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:18:59.358211Z",
     "iopub.status.busy": "2021-11-23T06:18:59.356843Z",
     "iopub.status.idle": "2021-11-23T06:19:01.110670Z",
     "shell.execute_reply": "2021-11-23T06:19:01.110088Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.462405Z"
    },
    "papermill": {
     "duration": 1.794496,
     "end_time": "2021-11-23T06:19:01.110838",
     "exception": false,
     "start_time": "2021-11-23T06:18:59.316342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1400, 3, 224, 224])\n",
      "torch.Size([1400, 3, 224, 224])\n",
      "torch.Size([1400])\n",
      "torch.Size([1400])\n"
     ]
    }
   ],
   "source": [
    "    x_train_typical = torch.tensor(x_train_typical).to(dtype=torch.float16)\n",
    "    x_train_atypical = torch.tensor(x_train_atypical).to(dtype=torch.float16)\n",
    "\n",
    "    y_train_typical = torch.tensor(y_train_typical, dtype=torch.long)\n",
    "    y_train_atypical = torch.tensor(y_train_atypical, dtype=torch.long)\n",
    "    \n",
    "    print(x_train_typical.shape)\n",
    "    print(x_train_atypical.shape)\n",
    "    print(y_train_typical.shape)\n",
    "    print(y_train_atypical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98291c",
   "metadata": {
    "papermill": {
     "duration": 0.0351,
     "end_time": "2021-11-23T06:19:01.179249",
     "exception": false,
     "start_time": "2021-11-23T06:19:01.144149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# to loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c55eec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:01.256127Z",
     "iopub.status.busy": "2021-11-23T06:19:01.254909Z",
     "iopub.status.idle": "2021-11-23T06:19:01.258573Z",
     "shell.execute_reply": "2021-11-23T06:19:01.258012Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.465066Z"
    },
    "papermill": {
     "duration": 0.045842,
     "end_time": "2021-11-23T06:19:01.258742",
     "exception": false,
     "start_time": "2021-11-23T06:19:01.212900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    trainloader = torch.utils.data.DataLoader(x_train, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    train_labels_loader = torch.utils.data.DataLoader(y_train, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(x_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    test_labels_loader = torch.utils.data.DataLoader(y_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    #validloader = torch.utils.data.DataLoader(x_valid, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    #valid_labels_loader = torch.utils.data.DataLoader(y_valid, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    submitloader = torch.utils.data.DataLoader(valid_data, batch_size=150, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fa584",
   "metadata": {
    "papermill": {
     "duration": 0.034081,
     "end_time": "2021-11-23T06:19:01.326112",
     "exception": false,
     "start_time": "2021-11-23T06:19:01.292031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# to loader (typ/atyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b449d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:01.401193Z",
     "iopub.status.busy": "2021-11-23T06:19:01.400136Z",
     "iopub.status.idle": "2021-11-23T06:19:01.402987Z",
     "shell.execute_reply": "2021-11-23T06:19:01.403642Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.467215Z"
    },
    "papermill": {
     "duration": 0.044545,
     "end_time": "2021-11-23T06:19:01.403835",
     "exception": false,
     "start_time": "2021-11-23T06:19:01.359290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    trainloader_typical = torch.utils.data.DataLoader(x_train_typical, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    trainloader_atypical = torch.utils.data.DataLoader(x_train_atypical, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    train_labels_loader_typical = torch.utils.data.DataLoader(y_train_typical, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    train_labels_loader_atypical = torch.utils.data.DataLoader(y_train_atypical, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd9527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:01.479764Z",
     "iopub.status.busy": "2021-11-23T06:19:01.478655Z",
     "iopub.status.idle": "2021-11-23T06:19:16.224722Z",
     "shell.execute_reply": "2021-11-23T06:19:16.224144Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.469256Z"
    },
    "papermill": {
     "duration": 14.786277,
     "end_time": "2021-11-23T06:19:16.224933",
     "exception": false,
     "start_time": "2021-11-23T06:19:01.438656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch) (1.9.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (3.10.0.2)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=aa2be28f9c5f0b1fde1018d2957da40c9df8b201b4b1ba82b1cc09f89ee22edc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\r\n",
      "Successfully built efficientnet-pytorch\r\n",
      "Installing collected packages: efficientnet-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install pretrainedmodels\n",
    "#!pip install albumentations\n",
    "\n",
    "!pip install --upgrade efficientnet-pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ad768",
   "metadata": {
    "papermill": {
     "duration": 0.036669,
     "end_time": "2021-11-23T06:19:16.298929",
     "exception": false,
     "start_time": "2021-11-23T06:19:16.262260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model & param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37993a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:16.432050Z",
     "iopub.status.busy": "2021-11-23T06:19:16.430899Z",
     "iopub.status.idle": "2021-11-23T06:19:23.741432Z",
     "shell.execute_reply": "2021-11-23T06:19:23.740787Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.471839Z"
    },
    "papermill": {
     "duration": 7.406019,
     "end_time": "2021-11-23T06:19:23.741608",
     "exception": false,
     "start_time": "2021-11-23T06:19:16.335589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33e6fd433b74d918bf80008626c8606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/74.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    #model = models.resnet101(pretrained=True).to(device, dtype=torch.float16)\n",
    "    \n",
    "    #model = models.resnet152(pretrained=True).to(device, dtype=torch.float16)\n",
    "    #model = models.efficientnet_b7(pretrained=True).to(device, dtype=torch.float16)\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=3).to(device, dtype=torch.float16)\n",
    "    \n",
    "    #num_ftrs = model.fc.in_features\n",
    "    #model.fc = nn.Linear(num_ftrs, 3).to(device, dtype=torch.float16)\n",
    "    #model.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 3)).to(device, dtype=torch.float16)\n",
    "    #print(model.fc)\n",
    "    \n",
    "    model.eval()\n",
    "    #if wb == 1: wandb.log({\"model\": 'resnet101 with pretrained'})\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bbf5d",
   "metadata": {
    "papermill": {
     "duration": 0.037065,
     "end_time": "2021-11-23T06:19:23.817463",
     "exception": false,
     "start_time": "2021-11-23T06:19:23.780398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# another two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f1eb4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:23.902774Z",
     "iopub.status.busy": "2021-11-23T06:19:23.901585Z",
     "iopub.status.idle": "2021-11-23T06:19:25.066522Z",
     "shell.execute_reply": "2021-11-23T06:19:25.065328Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.473890Z"
    },
    "papermill": {
     "duration": 1.212059,
     "end_time": "2021-11-23T06:19:25.066755",
     "exception": false,
     "start_time": "2021-11-23T06:19:23.854696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    model_typical = EfficientNet.from_pretrained('efficientnet-b4', num_classes=3).to(device, dtype=torch.float16)\n",
    "    model_atypical = EfficientNet.from_pretrained('efficientnet-b4', num_classes=3).to(device, dtype=torch.float16)\n",
    "    \n",
    "    #model_typical = models.resnet101(pretrained=True).to(device, dtype=torch.float16)\n",
    "    #model_atypical = models.resnet101(pretrained=True).to(device, dtype=torch.float16)\n",
    "    \n",
    "    #num_ftrs_typical = model_typical.fc.in_features\n",
    "    #num_ftrs_atypical = model_atypical.fc.in_features\n",
    "    #model_typical.fc = nn.Linear(num_ftrs_typical, 3).to(device, dtype=torch.float16)\n",
    "    #model_atypical.fc = nn.Linear(num_ftrs_atypical, 3).to(device, dtype=torch.float16)\n",
    "    \n",
    "    model_typical.eval()\n",
    "    model_atypical.eval()\n",
    "    \n",
    "    criterion_typical = nn.CrossEntropyLoss()\n",
    "    optimizer_typical = optim.SGD(model_typical.parameters(), lr=LR_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion_atypical = nn.CrossEntropyLoss()\n",
    "    optimizer_atypical = optim.SGD(model_atypical.parameters(), lr=LR_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5717acd",
   "metadata": {
    "papermill": {
     "duration": 0.055373,
     "end_time": "2021-11-23T06:19:25.267914",
     "exception": false,
     "start_time": "2021-11-23T06:19:25.212541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a951ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:19:25.379886Z",
     "iopub.status.busy": "2021-11-23T06:19:25.366510Z",
     "iopub.status.idle": "2021-11-23T06:36:21.398967Z",
     "shell.execute_reply": "2021-11-23T06:36:21.400296Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.476140Z"
    },
    "papermill": {
     "duration": 1016.092468,
     "end_time": "2021-11-23T06:36:21.400536",
     "exception": false,
     "start_time": "2021-11-23T06:19:25.308068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 1, Loss: 1.017, Train Accuray: 0.471\n",
      "Model2: Epoch: 1, Loss: 0.916, Train Accuray: 0.573\n",
      "Model3: Epoch: 1, Loss: 0.947, Train Accuray: 0.571\n",
      "Test F1 score:0.427, Test Loss:1.076, Test Accuracy：0.472\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 2, Loss: 0.820, Train Accuray: 0.624\n",
      "Model2: Epoch: 2, Loss: 0.659, Train Accuray: 0.713\n",
      "Model3: Epoch: 2, Loss: 0.702, Train Accuray: 0.687\n",
      "Test F1 score:0.510, Test Loss:1.056, Test Accuracy：0.533\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 3, Loss: 0.616, Train Accuray: 0.755\n",
      "Model2: Epoch: 3, Loss: 0.520, Train Accuray: 0.786\n",
      "Model3: Epoch: 3, Loss: 0.518, Train Accuray: 0.798\n",
      "Test F1 score:0.541, Test Loss:1.035, Test Accuracy：0.561\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 4, Loss: 0.378, Train Accuray: 0.864\n",
      "Model2: Epoch: 4, Loss: 0.374, Train Accuray: 0.851\n",
      "Model3: Epoch: 4, Loss: 0.356, Train Accuray: 0.872\n",
      "Test F1 score:0.562, Test Loss:1.019, Test Accuracy：0.581\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 5, Loss: 0.190, Train Accuray: 0.946\n",
      "Model2: Epoch: 5, Loss: 0.259, Train Accuray: 0.914\n",
      "Model3: Epoch: 5, Loss: 0.223, Train Accuray: 0.934\n",
      "Test F1 score:0.526, Test Loss:1.014, Test Accuracy：0.553\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 6, Loss: 0.102, Train Accuray: 0.975\n",
      "Model2: Epoch: 6, Loss: 0.183, Train Accuray: 0.949\n",
      "Model3: Epoch: 6, Loss: 0.148, Train Accuray: 0.967\n",
      "Test F1 score:0.556, Test Loss:1.008, Test Accuracy：0.569\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 7, Loss: 0.069, Train Accuray: 0.988\n",
      "Model2: Epoch: 7, Loss: 0.135, Train Accuray: 0.964\n",
      "Model3: Epoch: 7, Loss: 0.103, Train Accuray: 0.980\n",
      "Test F1 score:0.581, Test Loss:1.001, Test Accuracy：0.586\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 8, Loss: 0.042, Train Accuray: 0.993\n",
      "Model2: Epoch: 8, Loss: 0.097, Train Accuray: 0.980\n",
      "Model3: Epoch: 8, Loss: 0.089, Train Accuray: 0.980\n",
      "Test F1 score:0.616, Test Loss:0.998, Test Accuracy：0.617\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 9, Loss: 0.033, Train Accuray: 0.995\n",
      "Model2: Epoch: 9, Loss: 0.076, Train Accuray: 0.986\n",
      "Model3: Epoch: 9, Loss: 0.062, Train Accuray: 0.991\n",
      "Test F1 score:0.603, Test Loss:0.985, Test Accuracy：0.614\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 10, Loss: 0.032, Train Accuray: 0.994\n",
      "Model2: Epoch: 10, Loss: 0.051, Train Accuray: 0.994\n",
      "Model3: Epoch: 10, Loss: 0.045, Train Accuray: 0.991\n",
      "Test F1 score:0.628, Test Loss:0.983, Test Accuracy：0.628\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 11, Loss: 0.026, Train Accuray: 0.993\n",
      "Model2: Epoch: 11, Loss: 0.042, Train Accuray: 0.994\n",
      "Model3: Epoch: 11, Loss: 0.035, Train Accuray: 0.996\n",
      "Test F1 score:0.627, Test Loss:0.979, Test Accuracy：0.628\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 12, Loss: 0.024, Train Accuray: 0.995\n",
      "Model2: Epoch: 12, Loss: 0.045, Train Accuray: 0.994\n",
      "Model3: Epoch: 12, Loss: 0.033, Train Accuray: 0.997\n",
      "Test F1 score:0.634, Test Loss:0.977, Test Accuracy：0.636\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 13, Loss: 0.023, Train Accuray: 0.996\n",
      "Model2: Epoch: 13, Loss: 0.039, Train Accuray: 0.993\n",
      "Model3: Epoch: 13, Loss: 0.030, Train Accuray: 0.998\n",
      "Test F1 score:0.637, Test Loss:0.976, Test Accuracy：0.639\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 14, Loss: 0.023, Train Accuray: 0.995\n",
      "Model2: Epoch: 14, Loss: 0.034, Train Accuray: 0.995\n",
      "Model3: Epoch: 14, Loss: 0.027, Train Accuray: 0.997\n",
      "Test F1 score:0.638, Test Loss:0.976, Test Accuracy：0.639\n",
      "\n",
      "---------------------------------------------\n",
      "Model1: Epoch: 15, Loss: 0.019, Train Accuray: 0.997\n",
      "Model2: Epoch: 15, Loss: 0.038, Train Accuray: 0.991\n",
      "Model3: Epoch: 15, Loss: 0.027, Train Accuray: 0.996\n",
      "Test F1 score:0.640, Test Loss:0.978, Test Accuracy：0.642\n"
     ]
    }
   ],
   "source": [
    "    final_predict=[]\n",
    "    \n",
    "    for epoch in range(0, EPOCH):\n",
    "        if epoch == 10:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=LR_RATE/2, momentum=0.9, weight_decay=5e-4)\n",
    "            optimizer_typical = optim.SGD(model_typical.parameters(), lr=LR_RATE/2, momentum=0.9, weight_decay=5e-4)\n",
    "            optimizer_atypical = optim.SGD(model_atypical.parameters(), lr=LR_RATE/2, momentum=0.9, weight_decay=5e-4)\n",
    "        #print(optimizer)\n",
    "        final_predict=[]\n",
    "        print('\\n---------------------------------------------')\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for (i, data), labels_data in zip(enumerate(trainloader), train_labels_loader):\n",
    "            length = len(trainloader)\n",
    "            inputs, labels = data, labels_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(outputs, predicted)\n",
    "            #fk\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "            \n",
    "            \n",
    "            # print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "            # % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "        print('Model1: Epoch: %d, Loss: %.03f, Train Accuray: %.3f' % (epoch + 1, sum_loss / (i + 1), correct / total))\n",
    "        #\n",
    "        train_acc = correct / total\n",
    "        train_loss = sum_loss/(i + 1)\n",
    "        \n",
    "        ############################################################################################\n",
    "        #### model_typical & model_atypical\n",
    "        ###############################################\n",
    "        model_typical.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for (i, data), labels_data in zip(enumerate(trainloader_typical), train_labels_loader_typical):\n",
    "            length = len(trainloader_typical)\n",
    "            inputs, labels = data, labels_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_typical.zero_grad()\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = model_typical(inputs)\n",
    "            #print(inputs.shape, outputs.shape, labels.shape)\n",
    "            #print(outputs)\n",
    "            loss = criterion_typical(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_typical.step()\n",
    "\n",
    "            #\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(outputs, predicted)\n",
    "            #fk\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "            \n",
    "            \n",
    "            # print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "            # % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "        print('Model2: Epoch: %d, Loss: %.03f, Train Accuray: %.3f' % (epoch + 1, sum_loss / (i + 1), correct / total))\n",
    "        #\n",
    "        train_acc_typical = correct / total\n",
    "        train_loss_typical = sum_loss/(i + 1)\n",
    "        ######################################\n",
    "        ##model 3\n",
    "        model_atypical.train()\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for (i, data), labels_data in zip(enumerate(trainloader_atypical), train_labels_loader_atypical):\n",
    "            length = len(trainloader_atypical)\n",
    "            inputs, labels = data, labels_data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer_atypical.zero_grad()\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = model_atypical(inputs)\n",
    "            \n",
    "            loss = criterion_atypical(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_atypical.step()\n",
    "\n",
    "            #\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(outputs, predicted)\n",
    "            #fk\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "            \n",
    "            \n",
    "            # print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "            # % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "        print('Model3: Epoch: %d, Loss: %.03f, Train Accuray: %.3f' % (epoch + 1, sum_loss / (i + 1), correct / total))\n",
    "        #\n",
    "        train_acc_atypical = correct / total\n",
    "        train_loss_atypical = sum_loss/(i + 1)\n",
    "        \n",
    "        # Optional\n",
    "        ###########################\n",
    "        # test accuracy\n",
    "        ###########################\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            test_losses = []\n",
    "            for data, label_data in zip(testloader, test_labels_loader):\n",
    "                model.eval()\n",
    "                model_typical.eval()\n",
    "                model_atypical.eval()\n",
    "                images, labels = data, label_data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs1 = model(images)\n",
    "                outputs2 = model_typical(images)\n",
    "                outputs3 = model_atypical(images)\n",
    "                \n",
    "                outputs_softmax1 = torch.nn.functional.softmax(outputs1, dim=1)\n",
    "                outputs_softmax2 = torch.nn.functional.softmax(outputs2, dim=1)\n",
    "                outputs_softmax3 = torch.nn.functional.softmax(outputs3, dim=1)\n",
    "                \n",
    "                \n",
    "                all_outputs = (outputs_softmax1 + outputs_softmax2*0.5 + outputs_softmax3*0.5)/3  # add weight on model 2,3\n",
    "                \n",
    "                loss = criterion(all_outputs, labels)\n",
    "                \n",
    "                test_losses.append(loss.item())\n",
    "                _, predicted = torch.max(all_outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "                #print(predicted.cpu().detach().numpy())\n",
    "                final_predict = np.concatenate([final_predict, predicted.cpu().detach().numpy()])\n",
    "            #print(final_predict)\n",
    "            final_predict = np.array(final_predict)    \n",
    "            test_f1 = f1_score(y_test, final_predict, average='macro')\n",
    "            test_loss = np.average(test_losses)\n",
    "            \n",
    "            print('Test F1 score:%.3f, Test Loss:%.3f, Test Accuracy：%.3f' % (test_f1, test_loss, correct / total))\n",
    "            acc = 100. * correct / total\n",
    "\n",
    "            #if acc > best_acc:\n",
    "             #   best_acc = acc\n",
    "        test_acc = correct / total\n",
    "        test_loss = test_loss\n",
    "        \n",
    "        \n",
    "        if wb == 1:\n",
    "            wandb.log({\"Train Accuracy\": train_acc,\n",
    "                        \"Train loss\": train_loss,\n",
    "                        \"Test Accuracy\": test_acc,\n",
    "                        \"Test F1 score\": test_f1,\n",
    "                        \"Test Loss\": test_loss,\n",
    "                      })\n",
    "                        #\"Valid Accuracy\": valid_acc,\n",
    "                        #\"Valid loss\": valid_loss})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d9ea43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:36:21.536094Z",
     "iopub.status.busy": "2021-11-23T06:36:21.535018Z",
     "iopub.status.idle": "2021-11-23T06:36:25.183421Z",
     "shell.execute_reply": "2021-11-23T06:36:25.184384Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.478406Z"
    },
    "papermill": {
     "duration": 3.72408,
     "end_time": "2021-11-23T06:36:25.184645",
     "exception": false,
     "start_time": "2021-11-23T06:36:21.460565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:0.640, Test Loss:0.908, Test Accuracy：0.642\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "#with open('../input/models/the_model1.sav', 'rb') as f:\n",
    "#    model = pickle.load(f)\n",
    "#with open('../input/models/the_model2.sav', 'rb') as f:\n",
    "#    model_typical = pickle.load(f)\n",
    "#with open('../input/models/the_model3.sav', 'rb') as f:\n",
    "#    model_atypical = pickle.load(f)\n",
    "    \n",
    "final_predict=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_losses = []\n",
    "    for data, label_data in zip(testloader, test_labels_loader):\n",
    "        model.eval()\n",
    "        model_typical.eval()\n",
    "        model_atypical.eval()\n",
    "        images, labels = data, label_data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "        outputs1 = model(images)\n",
    "        outputs2 = model_typical(images)\n",
    "        outputs3 = model_atypical(images)\n",
    "                \n",
    "        outputs_softmax1 = torch.nn.functional.softmax(outputs1, dim=1)\n",
    "        outputs_softmax2 = torch.nn.functional.softmax(outputs2, dim=1)\n",
    "        outputs_softmax3 = torch.nn.functional.softmax(outputs3, dim=1)\n",
    "                \n",
    "                \n",
    "        all_outputs = (outputs_softmax1*2 + outputs_softmax2 + outputs_softmax3)/3  # add weight on model 2,3\n",
    "        #all_outputs[:,2]*=1.25\n",
    "        loss = criterion(all_outputs, labels)\n",
    "                \n",
    "        test_losses.append(loss.item())\n",
    "        _, predicted = torch.max(all_outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "                #print(predicted.cpu().detach().numpy())\n",
    "        final_predict = np.concatenate([final_predict, predicted.cpu().detach().numpy()])\n",
    "            #print(final_predict)\n",
    "    final_predict = np.array(final_predict)    \n",
    "    test_f1 = f1_score(y_test, final_predict, average='macro')\n",
    "    test_loss = np.average(test_losses)\n",
    "            \n",
    "    print('Test F1 score:%.3f, Test Loss:%.3f, Test Accuracy：%.3f' % (test_f1, test_loss, correct / total))\n",
    "    acc = 100. * correct / total\n",
    "\n",
    "            #if acc > best_acc:\n",
    "             #   best_acc = acc\n",
    "test_acc = correct / total\n",
    "test_loss = test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2d537",
   "metadata": {
    "papermill": {
     "duration": 0.059073,
     "end_time": "2021-11-23T06:36:25.305038",
     "exception": false,
     "start_time": "2021-11-23T06:36:25.245965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20586884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:36:25.438499Z",
     "iopub.status.busy": "2021-11-23T06:36:25.433010Z",
     "iopub.status.idle": "2021-11-23T06:36:25.925590Z",
     "shell.execute_reply": "2021-11-23T06:36:25.926459Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.480509Z"
    },
    "papermill": {
     "duration": 0.561343,
     "end_time": "2021-11-23T06:36:25.926722",
     "exception": false,
     "start_time": "2021-11-23T06:36:25.365379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  9 22]\n",
      " [16 76 28]\n",
      " [23 31 66]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "filename = 'the_model1.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "filename2 = 'the_model2.sav'\n",
    "pickle.dump(model_typical, open(filename2, 'wb'))\n",
    "filename3 = 'the_model3.sav'\n",
    "pickle.dump(model_atypical, open(filename3, 'wb'))\n",
    "\n",
    "#print(final_predict+final_predict_another)\n",
    "#print(final_predict_another)\n",
    "\n",
    "matrix = confusion_matrix(y_test, final_predict)\n",
    "print(matrix)\n",
    "\n",
    "if wb==1:\n",
    "    class_labels=['0_Negative', '1_Typical', '2_Atypical']\n",
    "    #wandb.init()\n",
    "    wandb.log({\"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(\n",
    "        y_test, final_predict,\n",
    "        labels=class_labels ) })\n",
    "            #class_names=['Negative', 'Typical', 'Atypical']) })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11ae64",
   "metadata": {
    "papermill": {
     "duration": 0.059643,
     "end_time": "2021-11-23T06:36:26.050461",
     "exception": false,
     "start_time": "2021-11-23T06:36:25.990818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# generate result to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e051e1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:36:26.186539Z",
     "iopub.status.busy": "2021-11-23T06:36:26.185339Z",
     "iopub.status.idle": "2021-11-23T06:36:27.484554Z",
     "shell.execute_reply": "2021-11-23T06:36:27.483462Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.483825Z"
    },
    "papermill": {
     "duration": 1.37281,
     "end_time": "2021-11-23T06:36:27.484812",
     "exception": false,
     "start_time": "2021-11-23T06:36:26.112002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 0,\n",
      "        0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 1,\n",
      "        0, 2, 1, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1,\n",
      "        0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2,\n",
      "        1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 2, 0, 2, 2, 0,\n",
      "        0, 2, 0, 1, 0, 1], device='cuda:0')\n",
      "Negative    58\n",
      "Typical     52\n",
      "Atypical    40\n",
      "Name: Type, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "    #if wb == 1: wandb.watch(model)\n",
    "    predict = 0\n",
    "    cnt=0\n",
    "    #print(len(submitloader))\n",
    "    with torch.no_grad():\n",
    "        for data in submitloader:\n",
    "            model.eval()\n",
    "            model_typical.eval()\n",
    "            model_atypical.eval()\n",
    "            \n",
    "            images = data\n",
    "            images = images.to(device)\n",
    "            outputs1 = model(images)\n",
    "            outputs2 = model_typical(images)\n",
    "            outputs3 = model_atypical(images)\n",
    "            \n",
    "            outputs_softmax1 = torch.nn.functional.softmax(outputs1, dim=1)\n",
    "            outputs_softmax2 = torch.nn.functional.softmax(outputs2, dim=1)\n",
    "            outputs_softmax3 = torch.nn.functional.softmax(outputs3, dim=1)\n",
    "            \n",
    "            all_outputs = (outputs_softmax1*2 + outputs_softmax2 + outputs_softmax3)/3  # add weight on model 2,3\n",
    "            \n",
    "            cnt+=1\n",
    "            print(cnt)\n",
    "    \n",
    "    #valid_data = valid_data.to(device)\n",
    "    #print(valid_data)\n",
    "    #outputs = model(valid_data)#.cpu().detach().numpy()\n",
    "    #print(outputs)\n",
    "            _, predict = torch.max(all_outputs.data, 1)\n",
    "    \n",
    "            #print(predict, predict2)\n",
    "    \n",
    "    \n",
    "            \n",
    "    print(predict)\n",
    "    #valid_dir = listdir('../input/case2-data/resized_valid_data/')\n",
    "    #valid_dir.sort()\n",
    "    \n",
    "    df_submit = pd.DataFrame(data={'FileID': valid_dir, 'Type': predict.cpu().detach().numpy()})\n",
    "    #df_submit = pd.DataFrame(data={'FileID': valid_dir, 'Type': merge_predict})\n",
    "    df_submit['FileID'] = df_submit['FileID'].str.replace('.jpg', '')\n",
    "    df_submit['Type'] = df_submit['Type'].replace(0, 'Negative')\n",
    "    df_submit['Type'] = df_submit['Type'].replace(1, 'Typical')\n",
    "    df_submit['Type'] = df_submit['Type'].replace(2, 'Atypical')\n",
    "\n",
    "    print(df_submit['Type'].value_counts())\n",
    "    df_submit.to_csv('./submit.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e7a642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T06:36:27.700141Z",
     "iopub.status.busy": "2021-11-23T06:36:27.698798Z",
     "iopub.status.idle": "2021-11-23T06:36:27.708710Z",
     "shell.execute_reply": "2021-11-23T06:36:27.709731Z",
     "shell.execute_reply.started": "2021-11-23T06:17:23.486897Z"
    },
    "papermill": {
     "duration": 0.154634,
     "end_time": "2021-11-23T06:36:27.710041",
     "exception": false,
     "start_time": "2021-11-23T06:36:27.555407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if wb == 1:\n",
    "    run = wandb.init(reinit=True)\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1113.370801,
   "end_time": "2021-11-23T06:36:30.574099",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-23T06:17:57.203298",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e685938b9c54c818757e62033f936f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "414714ecd6db4887a4722c03410e3975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e5f4a1b0b48a4d5abd4f6f656afe96c3",
       "placeholder": "​",
       "style": "IPY_MODEL_d800f84b549a45bc850a5a066f9a74bf",
       "value": " 74.4M/74.4M [00:00&lt;00:00, 145MB/s]"
      }
     },
     "7c41cc5d13264c0ab26c5f394e030127": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a9e53681956f4690bbf023835b10c01e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c92d27a078ac4b12a1a34cf81ebd9c30",
       "placeholder": "​",
       "style": "IPY_MODEL_cccbde61289340d3aa2213b6dfd5356e",
       "value": "100%"
      }
     },
     "b8523f2a49ed4f42a2ec82d21005285b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e685938b9c54c818757e62033f936f6",
       "max": 77999237.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7c41cc5d13264c0ab26c5f394e030127",
       "value": 77999237.0
      }
     },
     "c92d27a078ac4b12a1a34cf81ebd9c30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cccbde61289340d3aa2213b6dfd5356e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d800f84b549a45bc850a5a066f9a74bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e33e6fd433b74d918bf80008626c8606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a9e53681956f4690bbf023835b10c01e",
        "IPY_MODEL_b8523f2a49ed4f42a2ec82d21005285b",
        "IPY_MODEL_414714ecd6db4887a4722c03410e3975"
       ],
       "layout": "IPY_MODEL_f19fd3acdb77496aab77f88530b4c38d"
      }
     },
     "e5f4a1b0b48a4d5abd4f6f656afe96c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f19fd3acdb77496aab77f88530b4c38d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
